{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a1cfe7-fa5c-49fc-82bc-5f9bf79f0f2b",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cbfa64b1-accb-447b-a446-9d20ca3fe2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752bf845-9964-4ae8-b676-e2133ba7b182",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "daa6387e-4291-4fc2-afdc-30da6c867662",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '~/datasets'\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "x_dim  = 784\n",
    "hidden_dim = 400\n",
    "latent_dim = 200\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "input_channels = 1\n",
    "latent_dim = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97bc09e-2642-4329-a895-ecfa699b12fe",
   "metadata": {},
   "source": [
    "Define the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9d48d718-2a13-435f-a520-82f2611038bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devine the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=4, stride=2, padding=1)  # Output: (32, 14, 14)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)  # Output: (64, 7, 7)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mean = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        mean = self.fc_mean(x)\n",
    "        log_var = self.fc_log_var(x)\n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e5476-d18d-4d0c-86c2-d7ab4b642752",
   "metadata": {},
   "source": [
    "Define the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "424e33d4-faa4-4ae9-aaf9-22a0755f8960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 64 * 7 * 7)\n",
    "        self.deconv1 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)  # Output: (32, 14, 14)\n",
    "        self.deconv2 = nn.ConvTranspose2d(32, output_channels, kernel_size=4, stride=2, padding=1)  # Output: (1, 28, 28)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = F.relu(self.fc(z))\n",
    "        x = x.view(-1, 64, 7, 7)  # Reshape to spatial dimensions\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = torch.sigmoid(self.deconv2(x))  # Sigmoid to constrain outputs between 0 and 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a17fbb-2a87-4290-8243-dc753cca5cef",
   "metadata": {},
   "source": [
    "Define VAE Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0f3fc563-8d80-4db8-901a-b19bfb1ed752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_channels, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, input_channels)\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std).to(DEVICE)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encoder(x)\n",
    "        z = self.reparameterize(mean, log_var)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mean, log_var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b028e-d0c9-4a7a-8fb4-a0df6ce6397b",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7227ace1-23e5-4a3b-90ca-b5cef44ba577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(reconstructed, original, mean, log_var):\n",
    "    # Reconstruction loss\n",
    "    reconstruction_loss = F.binary_cross_entropy(reconstructed, original, reduction='sum')\n",
    "    # KL divergence\n",
    "    kl_divergence = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return reconstruction_loss + kl_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffdb205-c230-4d5c-9a74-3d5830426c44",
   "metadata": {},
   "source": [
    "Train on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8a3b0dbd-7745-4eaa-b55c-cb676261d019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 153.27779799804688\n",
      "Epoch 2, Loss: 111.11452986653646\n",
      "Epoch 3, Loss: 107.53707117513021\n",
      "Epoch 4, Loss: 105.77988727213541\n",
      "Epoch 5, Loss: 104.64051087239584\n",
      "Epoch 6, Loss: 103.86989080403646\n",
      "Epoch 7, Loss: 103.25198103841146\n",
      "Epoch 8, Loss: 102.694903125\n",
      "Epoch 9, Loss: 102.24201580403646\n",
      "Epoch 10, Loss: 101.88797638346354\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "\n",
    "# Define transformations\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(dataset_path, transform=mnist_transform, train=True, download=False)\n",
    "\n",
    "# Filter dataset\n",
    "#train_number = 0\n",
    "#filtered_images = []\n",
    "#filtered_labels = []\n",
    "#for image, label in train_dataset:\n",
    "    #if label == train_number:\n",
    "        #filtered_images.append(image)\n",
    "        #filtered_labels.append(label)\n",
    "#filtered_dataset = TensorDataset(torch.stack(filtered_images), torch.tensor(filtered_labels))\n",
    "\n",
    "# Create a new DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n",
    "\n",
    "# Define model\n",
    "vae = VAE(input_channels, latent_dim).to(DEVICE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "vae.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        images, _ = batch\n",
    "        images = images.to(torch.float32)\n",
    "        images = images.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed, mean, log_var = vae(images)\n",
    "        loss = loss_function(reconstructed, images, mean, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_dataset)}\")\n",
    "\n",
    "# Saving the model and optimizer\n",
    "torch.save({\n",
    "    'model_state_dict': vae.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, \"demo_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8e3234-d2fb-40e3-8fa4-b5bc4944ffec",
   "metadata": {},
   "source": [
    "Load and Run Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0599e516-97cc-45ed-a10a-5f91e8244371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aes\\AppData\\Local\\Temp\\ipykernel_29420\\3473606792.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"demo_checkpoint.pth\")\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.MNIST(dataset_path, transform=mnist_transform, train=False, download=False)\n",
    "# Filter test dataset based on digit label\n",
    "test_number = 0\n",
    "filtered_test_dataset = [(x, y) for x, y in test_dataset if y == test_number]\n",
    "test_loader  = DataLoader(dataset=filtered_test_dataset,  batch_size=batch_size, shuffle=False,  **kwargs) # Change value for dataset to generate different numbers\n",
    "\n",
    "try:\n",
    "    # Reloading the model and optimizer\n",
    "    checkpoint = torch.load(\"demo_checkpoint.pth\")\n",
    "    vae = VAE(input_channels=1, latent_dim=20)  # Reinitialize model\n",
    "    vae.load_state_dict(checkpoint['model_state_dict'])\n",
    "    vae.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)  # Reinitialize optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "except:\n",
    "    print(\"Failed to load model!\")\n",
    "\n",
    "vae.eval()\n",
    "test_images, _ = next(iter(test_loader))\n",
    "test_images = test_images.to(DEVICE)\n",
    "reconstructed, _, _ = vae(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426d941-d673-4bff-8fc2-e71d1354c7f9",
   "metadata": {},
   "source": [
    "Visualize Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bfac838d-e628-4e6b-b0b5-a2ee1409c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(original_batch, reconstructed_batch, index):\n",
    "    # Extract the specific image and move to CPU\n",
    "    original = original_batch[index].detach().cpu().numpy().squeeze()\n",
    "    reconstructed = reconstructed_batch[index].detach().cpu().numpy().squeeze()\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(original, cmap='gray')\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Reconstructed image\n",
    "    axes[1].imshow(reconstructed, cmap='gray')\n",
    "    axes[1].set_title(\"Generated\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "aad4d8aa-10be-4fbf-872d-9a8917657803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(test_images, reconstructed, index=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
