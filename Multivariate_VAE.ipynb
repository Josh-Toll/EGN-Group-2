{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773bff8d-e5d8-4346-853a-6c58cd18bb5b",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9a3544-50ce-428d-bb67-ac17a7a53493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from preprocessing import ImageDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3cf049-314d-4921-ab06-8517851a043b",
   "metadata": {},
   "source": [
    "Hyperperameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb7ad0f-af8d-4dde-b299-cd043d8a65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} \n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "x_dim  = 784\n",
    "hidden_dim = 400\n",
    "latent_dim = 200\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "input_channels = 3\n",
    "latent_dim = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27ead8b-9636-4d52-b9ff-2d640b7fc1bb",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "625bc165-f6b8-4f97-825e-57332ec3ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=4, stride=2, padding=1)  # (32, 128, 128)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)  # (64, 64, 64)\n",
    "        self.conv3 = nn.Conv2d(64, 32, kernel_size=4, stride=2, padding=1)  # (32, 32, 32)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Output mean vector\n",
    "        self.fc_mean = nn.Linear(32 ** 3, latent_dim)\n",
    "        \n",
    "        # Output covariance matrix (as a flattened vector)\n",
    "        self.fc_cov = nn.Linear(32 ** 3, latent_dim * latent_dim)\n",
    "\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        mean = self.fc_mean(x)\n",
    "        cov_flat = self.fc_cov(x)\n",
    "        cov_matrix = cov_flat.view(-1, mean.size(1), mean.size(1))  # Reshape to square covariance matrix\n",
    "\n",
    "        cov_matrix = 0.5 * (cov_matrix + cov_matrix.transpose(-1, -2))  # Ensure symmetry\n",
    "        cov_matrix = cov_matrix + 1e-4 * torch.eye(latent_dim).to(cov_matrix.device)  # Ensure positive definiteness\n",
    "        \n",
    "        # Ensure the covariance matrix is positive-definite\n",
    "        cov_matrix = torch.clamp(cov_matrix, min=1e-4)  # Prevent it from becoming too small\n",
    "        \n",
    "        return mean, cov_matrix\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 32 ** 3)\n",
    "        self.deconv1 = nn.ConvTranspose2d(32, 64, kernel_size=4, stride=2, padding=1) # (64, 64, 64)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)  # (32, 128, 128)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, output_channels, kernel_size=4, stride=2, padding=1)  # (3, 256, 256)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = F.relu(self.fc(z))\n",
    "        x = x.view(-1, 32, 32, 32)  # Reshape to spatial dimensions\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = torch.sigmoid(self.deconv2(x))\n",
    "        x = torch.sigmoid(self.deconv3(x))\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_channels, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, input_channels)\n",
    "\n",
    "    def reparameterize(self, mean, cov_matrix):\n",
    "        batch_size, latent_dim = mean.shape\n",
    "    \n",
    "        # Ensure positive-definiteness and numerical stability\n",
    "        cov_matrix = cov_matrix + 1e-4 * torch.eye(latent_dim).to(cov_matrix.device)  # Ensure positive definiteness\n",
    "        cov_matrix = torch.clamp(cov_matrix, min=1e-4)  # Prevent it from becoming too small\n",
    "    \n",
    "        # Use Cholesky decomposition to obtain L such that L * L^T = cov_matrix\n",
    "        L = torch.linalg.cholesky(cov_matrix)\n",
    "    \n",
    "        # Sample from standard normal\n",
    "        epsilon = torch.randn(batch_size, latent_dim).to(cov_matrix.device)\n",
    "    \n",
    "        # Reparameterization trick: z = mean + L * epsilon\n",
    "        return mean + torch.matmul(L, epsilon.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, cov_matrix = self.encoder(x)\n",
    "        z = self.reparameterize(mean, cov_matrix)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mean, cov_matrix\n",
    "\n",
    "\n",
    "def loss_function(reconstructed, original, mean, cov_matrix):\n",
    "    # Reconstruction loss\n",
    "    reconstruction_loss = F.mse_loss(reconstructed, original, reduction='sum')\n",
    "    \n",
    "    # KL Divergence for Multivariate Gaussian\n",
    "    batch_size, latent_dim, _ = cov_matrix.size()\n",
    "    \n",
    "    cov_trace = torch.diagonal(cov_matrix, dim1=-2, dim2=-1).sum(dim=-1)  # Tr(cov)\n",
    "    cov_det = torch.linalg.det(cov_matrix) + 1e-6  # Prevent log of zero\n",
    "    \n",
    "    # Avoid log(0) or negative determinant\n",
    "    kl_divergence = 0.5 * (torch.sum(mean ** 2, dim=-1) + cov_trace - latent_dim - torch.log(cov_det))\n",
    "    kl_divergence = kl_divergence.sum()\n",
    "\n",
    "    return reconstruction_loss + kl_divergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a0267-a089-4943-8395-bf401fcdf6d9",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb88d423-9d58-4643-848d-2052c43a497b",
   "metadata": {},
   "outputs": [
    {
     "ename": "_LinAlgError",
     "evalue": "linalg.cholesky: (Batch element 0): The factorization could not be completed because the input is not positive-definite (the leading minor of order 3 is not positive-definite).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 27\u001b[0m reconstructed, mean, cov_matrix \u001b[38;5;241m=\u001b[39m vae(images)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(reconstructed, images, mean, cov_matrix)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\SeniorDesignProject\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\SeniorDesignProject\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[54], line 76\u001b[0m, in \u001b[0;36mVAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     75\u001b[0m     mean, cov_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m---> 76\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mean, cov_matrix)\n\u001b[0;32m     77\u001b[0m     reconstructed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reconstructed, mean, cov_matrix\n",
      "Cell \u001b[1;32mIn[54], line 66\u001b[0m, in \u001b[0;36mVAE.reparameterize\u001b[1;34m(self, mean, cov_matrix)\u001b[0m\n\u001b[0;32m     63\u001b[0m cov_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(cov_matrix, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)  \u001b[38;5;66;03m# Prevent it from becoming too small\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Use Cholesky decomposition to obtain L such that L * L^T = cov_matrix\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m L \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky(cov_matrix)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Sample from standard normal\u001b[39;00m\n\u001b[0;32m     69\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(batch_size, latent_dim)\u001b[38;5;241m.\u001b[39mto(cov_matrix\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31m_LinAlgError\u001b[0m: linalg.cholesky: (Batch element 0): The factorization could not be completed because the input is not positive-definite (the leading minor of order 3 is not positive-definite)."
     ]
    }
   ],
   "source": [
    "image_dataset = ImageDataset(\"map_images_original/labels.csv\", \"map_images_original/\")\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "indices = list(range(len(image_dataset)))\n",
    "train_labels, test_labels = train_test_split(indices, test_size=0.2, random_state=42)  # 80% train, 20% test\n",
    "train_dataset = Subset(image_dataset, train_labels)\n",
    "test_dataset = Subset(image_dataset, test_labels)\n",
    "\n",
    "# Create a new DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n",
    "\n",
    "# Define model\n",
    "vae = VAE(input_channels, latent_dim).to(DEVICE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "vae.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, _) in enumerate(train_loader):\n",
    "        images = images.to(torch.float32)\n",
    "        images = images.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed, mean, cov_matrix = vae(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(reconstructed, images, mean, cov_matrix)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / (batch_idx * batch_size)}\")\n",
    "# Saving the model and optimizer\n",
    "torch.save({\n",
    "    'model_state_dict': vae.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, \"MVVAE_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5536b17-15bb-47b2-a0d3-a50411bb2826",
   "metadata": {},
   "source": [
    "Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9be903-b5f4-4bed-ab90-8f3f34a78490",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False,  **kwargs)\n",
    "\n",
    "# Reloading the model and optimizer\n",
    "checkpoint = torch.load(\"MVVAE_checkpoint.pth\")\n",
    "vae = VAE(input_channels=input_channels, latent_dim=latent_dim)  # Reinitialize model\n",
    "vae.load_state_dict(checkpoint['model_state_dict'])\n",
    "vae.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=lr)  # Reinitialize optimizer\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "vae.eval()\n",
    "for batch_idx, (test_images, _) in enumerate(test_loader):\n",
    "    test_images = test_images.to(DEVICE)\n",
    "    reconstructed, mean, log_var = vae(test_images)\n",
    "    loss = loss_function(reconstructed, test_images, mean, log_var)\n",
    "print(loss.item() / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896e249-0dca-4f1a-8891-6d3132bd4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(original_batch, reconstructed_batch, index):\n",
    "    # Extract the specific image and move to CPU\n",
    "    original = original_batch[index].detach().cpu()\n",
    "    reconstructed = reconstructed_batch[index].detach().cpu()\n",
    "\n",
    "    transform = T.Compose([\n",
    "    T.ToPILImage()\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    original = transform(original)\n",
    "    reconstructed = transform(reconstructed)\n",
    "    \n",
    "        \n",
    "    # Create the plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(original)\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Reconstructed image\n",
    "    axes[1].imshow(reconstructed)\n",
    "    axes[1].set_title(\"Generated\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_image(test_images, reconstructed, index=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dc68f8-3139-4ecd-9b05-eba8e9fd2f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
