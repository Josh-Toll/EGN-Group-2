{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a1cfe7-fa5c-49fc-82bc-5f9bf79f0f2b",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbfa64b1-accb-447b-a446-9d20ca3fe2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752bf845-9964-4ae8-b676-e2133ba7b182",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa6387e-4291-4fc2-afdc-30da6c867662",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '~/datasets'\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "x_dim  = 784\n",
    "hidden_dim = 400\n",
    "latent_dim = 200\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "input_channels = 1\n",
    "latent_dim = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97bc09e-2642-4329-a895-ecfa699b12fe",
   "metadata": {},
   "source": [
    "Define the Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d48d718-2a13-435f-a520-82f2611038bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devine the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=4, stride=2, padding=1)  # Output: (32, 14, 14)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)  # Output: (64, 7, 7)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mean = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "\n",
    "        self.relu = nn.LeakyReLU(0.2)\n",
    "\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        mean = self.fc_mean(x)\n",
    "        log_var = self.fc_log_var(x)\n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e5476-d18d-4d0c-86c2-d7ab4b642752",
   "metadata": {},
   "source": [
    "Define the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "424e33d4-faa4-4ae9-aaf9-22a0755f8960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, 64 * 7 * 7)\n",
    "        self.deconv1 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)  # Output: (32, 14, 14)\n",
    "        self.deconv2 = nn.ConvTranspose2d(32, output_channels, kernel_size=4, stride=2, padding=1)  # Output: (1, 28, 28)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = F.relu(self.fc(z))\n",
    "        x = x.view(-1, 64, 7, 7)  # Reshape to spatial dimensions\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = torch.sigmoid(self.deconv2(x))  # Sigmoid to constrain outputs between 0 and 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a17fbb-2a87-4290-8243-dc753cca5cef",
   "metadata": {},
   "source": [
    "Define VAE Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f3fc563-8d80-4db8-901a-b19bfb1ed752",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_channels, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, input_channels)\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std).to(DEVICE)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encoder(x)\n",
    "        z = self.reparameterize(mean, log_var)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mean, log_var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b028e-d0c9-4a7a-8fb4-a0df6ce6397b",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7227ace1-23e5-4a3b-90ca-b5cef44ba577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(reconstructed, original, mean, log_var):\n",
    "    # Reconstruction loss\n",
    "    reconstruction_loss = F.binary_cross_entropy(reconstructed, original, reduction='sum')\n",
    "    # KL divergence\n",
    "    kl_divergence = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    return reconstruction_loss + kl_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffdb205-c230-4d5c-9a74-3d5830426c44",
   "metadata": {},
   "source": [
    "Train on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a3b0dbd-7745-4eaa-b55c-cb676261d019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 291.4534118848134\n",
      "Epoch 2, Loss: 203.02327394268107\n",
      "Epoch 3, Loss: 192.5661358528195\n",
      "Epoch 4, Loss: 186.44414814336062\n",
      "Epoch 5, Loss: 183.612639881078\n",
      "Epoch 6, Loss: 181.03897676430861\n",
      "Epoch 7, Loss: 178.92096521241348\n",
      "Epoch 8, Loss: 176.99658771948336\n",
      "Epoch 9, Loss: 175.44819222997214\n",
      "Epoch 10, Loss: 174.0029308416343\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "\n",
    "# Define transformations\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(dataset_path, transform=mnist_transform, train=True, download=False)\n",
    "\n",
    "# Filter dataset\n",
    "train_number = 1\n",
    "filtered_images = []\n",
    "filtered_labels = []\n",
    "for image, label in train_dataset:\n",
    "    if label == train_number:\n",
    "        filtered_images.append(image)\n",
    "        filtered_labels.append(label)\n",
    "filtered_dataset_0 = TensorDataset(torch.stack(filtered_images), torch.tensor(filtered_labels))\n",
    "train_number = 0\n",
    "filtered_images_1 = []\n",
    "filtered_labels_1 = []\n",
    "for image, label in train_dataset:\n",
    "    if label == train_number:\n",
    "        filtered_images_1.append(image)\n",
    "        filtered_labels_1.append(label)\n",
    "filtered_dataset_1 = TensorDataset(torch.stack(filtered_images_1), torch.tensor(filtered_labels_1))\n",
    "    \n",
    "blank_dataset = torch.zeros((len(filtered_dataset_0), * (1, 28, 28)), dtype=torch.uint8)\n",
    "combined_labels = [label for (_, label), (_, label2) in zip(filtered_dataset_0, filtered_dataset_1)]\n",
    "layered_dataset = [(torch.cat((img1, img2), dim=0), combined_labels) for (img1, label), (img2, label2) in zip(filtered_dataset_0, filtered_dataset_1)]\n",
    "layered_images = []\n",
    "layered_labels = []\n",
    "for image, label in layered_dataset:\n",
    "    image = torch.mean(image, axis=0, keepdims=True)\n",
    "    layered_images.append(image)\n",
    "    layered_labels.append(label)\n",
    "layered_dataset = TensorDataset(torch.stack(layered_images), torch.tensor(layered_labels))\n",
    "\n",
    "# Create a new DataLoader\n",
    "train_loader = DataLoader(layered_dataset, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n",
    "\n",
    "# Create a new DataLoader\n",
    "train_loader = DataLoader(layered_dataset, batch_size=batch_size, shuffle=True, drop_last=True, **kwargs)\n",
    "\n",
    "# Define model\n",
    "vae = VAE(input_channels, latent_dim).to(DEVICE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "vae.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch\n",
    "        images = images.to(torch.float32)\n",
    "        images = images.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed, mean, log_var = vae(images)\n",
    "        loss = loss_function(reconstructed, images, mean, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(layered_dataset)}\")\n",
    "\n",
    "# Saving the model and optimizer\n",
    "torch.save({\n",
    "    'model_state_dict': vae.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}, \"demo_layered_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8e3234-d2fb-40e3-8fa4-b5bc4944ffec",
   "metadata": {},
   "source": [
    "Load and Run Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0599e516-97cc-45ed-a10a-5f91e8244371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aes\\AppData\\Local\\Temp\\ipykernel_20752\\1078430023.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"demo_layered_checkpoint.pth\")\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.MNIST(dataset_path, transform=mnist_transform, train=False, download=False)\n",
    "# Filter test dataset based on digit label\n",
    "test_number = 0\n",
    "filtered_test_dataset_0 = [(x, y) for x, y in test_dataset if y == test_number]\n",
    "test_number = 1\n",
    "filtered_test_dataset_1 = [(x, y) for x, y in test_dataset if y == test_number]\n",
    "blank_dataset = torch.zeros((len(filtered_test_dataset_0), * (1, 28, 28)), dtype=torch.uint8)\n",
    "\n",
    "combined_labels = [label for (_, label) in filtered_test_dataset_0]\n",
    "print(combined_labels[0])\n",
    "layered_dataset = [(torch.cat((img1, img2), dim=0), combined_labels) for (img1, label), (img2) in zip(filtered_test_dataset_0, blank_dataset)]\n",
    "layered_images = []\n",
    "layered_labels = []\n",
    "for image, label in layered_dataset:\n",
    "    image = torch.sum(image, axis=0, keepdims=True)\n",
    "    layered_images.append(image)\n",
    "    layered_labels.append(label)\n",
    "layered_dataset = TensorDataset(torch.stack(layered_images), torch.tensor(layered_labels))\n",
    "test_loader  = DataLoader(dataset=layered_dataset,  batch_size=batch_size, shuffle=False,  **kwargs) # Change value for dataset to generate different numbers\n",
    "\n",
    "try:\n",
    "    # Reloading the model and optimizer\n",
    "    checkpoint = torch.load(\"demo_layered_checkpoint.pth\")\n",
    "    vae = VAE(input_channels=1, latent_dim=20)  # Reinitialize model\n",
    "    vae.load_state_dict(checkpoint['model_state_dict'])\n",
    "    vae.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)  # Reinitialize optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "except:\n",
    "    print(\"Failed to load model!\")\n",
    "\n",
    "\n",
    "vae.eval()\n",
    "test_images, _ = next(iter(test_loader))\n",
    "test_images = test_images.to(DEVICE)\n",
    "reconstructed, _, _ = vae(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426d941-d673-4bff-8fc2-e71d1354c7f9",
   "metadata": {},
   "source": [
    "Visualize Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfac838d-e628-4e6b-b0b5-a2ee1409c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(original_batch, reconstructed_batch, index, method=\"sum\"):\n",
    "    # Extract the specific image and move to CPU\n",
    "    original = original_batch[index].detach().cpu().numpy()\n",
    "    reconstructed = reconstructed_batch[index].detach().cpu().numpy()\n",
    "    \n",
    "    # Average along the first dimension to get (1, 28, 28)\n",
    "    #if method == \"mean\":\n",
    "        #original = np.mean(original, axis=0, keepdims=True)\n",
    "        #reconstructed = np.mean(reconstructed, axis=0, keepdims=True)\n",
    "    #if method == \"sum\":\n",
    "        #original = np.sum(original, axis=0, keepdims=True)\n",
    "        #reconstructed = np.sum(reconstructed, axis=0, keepdims=True)\n",
    "    \n",
    "    # Remove the singleton dimension for visualization if needed\n",
    "    original = original.squeeze(0)\n",
    "    reconstructed = reconstructed.squeeze(0)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(original, cmap='gray')\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Reconstructed image\n",
    "    axes[1].imshow(reconstructed, cmap='gray')\n",
    "    axes[1].set_title(\"Generated\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aad4d8aa-10be-4fbf-872d-9a8917657803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "show_image(test_images, reconstructed, random.randint(0, len(test_images) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf8c0d75-f68d-447b-92b1-9dadc5b26aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n",
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "img = test_images[0].cpu().numpy()\n",
    "print(img.shape)\n",
    "img2 = np.sum(img, axis=0, keepdims=True)\n",
    "print(img2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72354383-9c14-44fe-9da6-915c57310558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
